## Project Overview

This project explores methods to explain the behavior of machine learning classifiers by interpreting their predictions. Specifically, it uses **LIME** (Local Interpretable Model-Agnostic Explanations) to explain predictions of a text classification model on individual instances.

#### What is LIME?
- **Local**: LIME creates a localized neighborhood around each instance to generate explanations specific to that instance.
- **Interpretable**: The explanations are designed to be easily understandable by humans.
- **Model-Agnostic**: LIME can be applied to any model, regardless of its underlying structure.
- **Explanations**: LIME provides insights that help in understanding model behavior and interpreting predictions.

### Text Classification:

<img width="586" alt="Screenshot 2024-11-08 at 10 17 45 PM" src="https://github.com/user-attachments/assets/fcbf3b5b-89da-4ddc-b678-8a59a7185e94">


<img width="290" alt="Screenshot 2024-11-08 at 10 18 44 PM" src="https://github.com/user-attachments/assets/40f093e3-3175-472a-8e50-2bc400837ac1">
